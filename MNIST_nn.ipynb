{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading from given archive file both the training and the test sets\n",
    "\n",
    "dataset = datasets.MNIST(root=\"/archive\", download=False, train=True, transform=ToTensor())\n",
    "#data_loaded = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "test_data = datasets.MNIST(root=\"/archive\", download=False, train=False, transform=ToTensor())\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying features and targets\n",
    "x = dataset.data\n",
    "y = dataset.targets\n",
    "#Splitting into train set and validation set\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "train = TensorDataset(x_train,y_train)\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "\n",
    "valid = TensorDataset(x_valid,y_valid)\n",
    "val_loader = DataLoader(valid, batch_size=32)\n",
    "\n",
    "#Dataloader was used to create batch sizes -->32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Neural network Model Architecture (Class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class containing constructor and forward_prop function \n",
    "#Constructor --> defining input/hidden/output layers and the realtion between them\n",
    "#Forward_prop --> defining the activation function for forward pass (relu)\n",
    "\n",
    "#using CNN because there are some layers that don't need fully connected layers -->locally connected only \n",
    "#using CNN for processing image so it won't take a long processing time like regular neural networks\n",
    "#using pooling decreases size of the convulotion output\n",
    "\n",
    "#Image --> Convolution --> Pooling --> Flatten --> Fully connected\n",
    "\n",
    "class neural_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(neural_net,self).__init__()\n",
    "        #defining architecture\n",
    "\n",
    "        # 1) Convolution Layer\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "\n",
    "        #bonus -->drop out layer (ignores random neural so not all of it is used in training)\n",
    "        #Doesn't change the shape of the data\n",
    "        self.dropout2 = nn.Dropout2d()\n",
    "\n",
    "        #Fully Connected layers\n",
    "        #320-->150-->80-->30-->10\n",
    "        # In layer --> 320 neurons\n",
    "        # hidden1 --> 150 neurons\n",
    "        # hidden2 --> 80 neurons\n",
    "        # hidden3 --> 30 neurons\n",
    "        # out layer --> 10 neurons (representing 0-->9)\n",
    "        self.fc1 = nn.Linear(320,150)\n",
    "        self.fc2 = nn.Linear(150,80)\n",
    "        self.fc3 = nn.Linear(80,30)\n",
    "        self.out = nn.Linear(30,10)\n",
    "        \n",
    "\n",
    "    #forward propagation function    \n",
    "    def forward(self,x):\n",
    "        # 2) Pooling Layer\n",
    "        #using ReLu Activation function\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),2))\n",
    "        x = F.relu(F.max_pool2d(self.dropout2(self.conv2(x)), 2))\n",
    "\n",
    "        # 3) FLatten the data --> for the linear layers\n",
    "        x.view(-1,320) # -1 --> so we can change batch sizes\n",
    "\n",
    "        # 4) Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #another dropout layer --> for regularization\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "\n",
    "        return F.log_softmax(x) #--> returns probability of each digit\n",
    "    \n",
    "\n",
    "#create model instance\n",
    "model = neural_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Optimizer,Learning Rate,Loss Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate --> 0.01 (the smaller lr is the longer training time)\n",
    "#Op --> SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "#Loss --> Cross Entropy\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model (Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rahma\\OneDrive\\Desktop\\ML\\Assig-2\\MNIST_nn.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,  \u001b[39m5\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     training(epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#If the loss doesn't decrease in output --> smaller learning rate\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\rahma\\OneDrive\\Desktop\\ML\\Assig-2\\MNIST_nn.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_num,(data,target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     training_loss \u001b[39m=\u001b[39m loss(output,target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     training_loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\rahma\\OneDrive\\Desktop\\ML\\Assig-2\\MNIST_nn.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# 2) Pooling Layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#using ReLu Activation function\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x),\u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)), \u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rahma/OneDrive/Desktop/ML/Assig-2/MNIST_nn.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# 3) FLatten the data --> for the linear layers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "#Array to track training\n",
    "train_loss = []\n",
    "train_correct = []\n",
    "\n",
    "validation_loss =[]\n",
    "validtaion_correct =[]\n",
    "\n",
    "\n",
    "#Epochs Number (no. of runs on training set)\n",
    "\n",
    "\n",
    "#Train/Validation loop\n",
    "\n",
    "def training(epoch):\n",
    "    model.train()\n",
    "    for batch_num,(data,target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        training_loss = loss(output,target)\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "for epoch in range(1,  5):\n",
    "    training(epoch)\n",
    "#If the loss doesn't decrease in output --> smaller learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test And count how many was correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
