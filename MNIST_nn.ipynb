{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading from given archive file both the training and the test sets\n",
    "\n",
    "dataset = datasets.MNIST(root=\"/archive\", download=False, train=True, transform=ToTensor())\n",
    "\n",
    "test_data = datasets.MNIST(root=\"/archive\", download=False, train=False, transform=ToTensor())\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying features and targets\n",
    "x = dataset.data\n",
    "y = dataset.targets\n",
    "#Splitting into train set and validation set\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(x_train,y_train)\n",
    "\n",
    "valid_data = TensorDataset(x_valid,y_valid)\n",
    "\n",
    "#Dataloader was used to create batch sizes -->32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,400).float()/255.0\n",
    "x_valid = x_valid.reshape(-1,400).float()/255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Neural network Model Architecture (Class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class containing constructor and forward_prop function \n",
    "#Constructor --> defining input/hidden/output layers and the realtion between them\n",
    "#Forward_prop --> defining the activation function for forward pass (relu)\n",
    "\n",
    "class neural_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #defining architecture\n",
    "\n",
    "        #Fully Connected layers\n",
    "        #400-->280-->150-->80-->30-->10\n",
    "\n",
    "        # In layer --> 400 neurons\n",
    "        # hidden1 --> 280 neurons\n",
    "        # hidden2 --> 150 neurons\n",
    "        # hidden3 --> 80 neurons\n",
    "        # hidden4 --> 30 neurons\n",
    "        # out layer --> 10 neurons (representing 0-->9)\n",
    "        self.fc1 = nn.Linear(400,280)\n",
    "        self.fc2 = nn.Linear(280,150)\n",
    "        self.fc3 = nn.Linear(150,80)\n",
    "        self.fc4 = nn.Linear(80,30)\n",
    "        self.out = nn.Linear(30,10)\n",
    "        \n",
    "\n",
    "    #forward propagation function    \n",
    "    def forward(self,x):\n",
    "        #using ReLu Activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #Bonus: dropout layer --> for regularization\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "\n",
    "        return F.softmax(x) #--> returns probability of each digit\n",
    "    \n",
    "\n",
    "#create model instance\n",
    "model = neural_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Optimizer,Learning Rate,Loss Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate --> 0.01 (the smaller lr is the longer training time)\n",
    "#Op --> SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "#Loss --> Cross Entropy\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array to track training\n",
    "train_loss_array = []\n",
    "train_correct_array = []\n",
    "\n",
    "validation_loss_array =[]\n",
    "validtaion_correct_array =[]\n",
    "\n",
    "\n",
    "#Epochs Number (no. of runs on training set)\n",
    "def loading(b):\n",
    "    global train_loader\n",
    "    train_loader= DataLoader(list(zip(x_train,y_train)),shuffle=True, batch_size=b)\n",
    "    global valid_loader \n",
    "    valid_loader= DataLoader(list(zip(x_valid,y_valid)),shuffle=False, batch_size=b)\n",
    "\n",
    " \n",
    "#If the loss doesn't decrease in output --> smaller learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch,b):\n",
    "    model.train()\n",
    "    for i,(x_batch ,y_batch) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        trainingloss = loss(y_pred,y_batch)\n",
    "        train_loss_array.append(trainingloss.item())\n",
    "        \n",
    "        trainingloss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {}: Batch: {}, Training Loss: {}'.format(epoch + 1, b, np.mean(train_loss_array)))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validating(b):\n",
    "    model.eval()\n",
    "    for i,(x_batch, y_batch) in enumerate(valid_loader,0):\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "        validloss = loss(y_pred,y_batch)\n",
    "        validation_loss_array.append(validloss.item())\n",
    "\n",
    "        validloss.backward()\n",
    "    print('          Batch: {}, Validation Loss: {}'.format(b, np.mean(validation_loss_array)))\n",
    "    print('-------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model and evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahma\\AppData\\Local\\Temp\\ipykernel_39540\\1774042978.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x) #--> returns probability of each digit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1: Batch: 32, Training Loss: 2.3023904250462848\n",
      "Epoch: 2: Batch: 32, Training Loss: 2.3023361012935637\n",
      "Epoch: 3: Batch: 32, Training Loss: 2.3022822767363653\n",
      "          Batch: 32, Validation Loss: 2.302283129374186\n",
      "-------------------------------------------\n",
      "Epoch: 1: Batch: 64, Training Loss: 2.302256688390459\n",
      "Epoch: 2: Batch: 64, Training Loss: 2.3022319426139197\n",
      "Epoch: 3: Batch: 64, Training Loss: 2.302207581449438\n",
      "          Batch: 64, Validation Loss: 2.302248960701653\n",
      "-------------------------------------------\n",
      "Epoch: 1: Batch: 128, Training Loss: 2.302195402580395\n",
      "Epoch: 2: Batch: 128, Training Loss: 2.3021834071795144\n",
      "Epoch: 3: Batch: 128, Training Loss: 2.3021716345529706\n",
      "          Batch: 128, Validation Loss: 2.3022334829675914\n",
      "-------------------------------------------\n",
      "Epoch: 1: Batch: 256, Training Loss: 2.3021658556818827\n",
      "Epoch: 2: Batch: 256, Training Loss: 2.3021601100586904\n",
      "Epoch: 3: Batch: 256, Training Loss: 2.3021544271883405\n",
      "          Batch: 256, Validation Loss: 2.302225825800137\n",
      "-------------------------------------------\n",
      "Epoch: 1: Batch: 512, Training Loss: 2.302151539255024\n",
      "Epoch: 2: Batch: 512, Training Loss: 2.3021486221714533\n",
      "Epoch: 3: Batch: 512, Training Loss: 2.3021457433673223\n",
      "          Batch: 512, Validation Loss: 2.3022213597874064\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "for batch_size in batch_sizes:\n",
    "    loading(batch_size)\n",
    "    for epoch in range(3):\n",
    "        training(epoch,batch_size)\n",
    "    validating(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test And count how many was correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
